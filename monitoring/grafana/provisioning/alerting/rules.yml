apiVersion: 1

groups:
  - name: CAPSIM Performance Alerts
    folder: CAPSIM
    interval: 1m
    rules:
      - uid: cpu-temperature-high
        title: "ðŸŒ¡ï¸ High CPU Temperature"
        condition: A
        for: 2m
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: "High CPU temperature detected ({{ $values.A.Value }}Â°C)"
          description: |
            CPU temperature is {{ $values.A.Value }}Â°C, which is above the threshold of 85Â°C.
            
            This indicates potential thermal throttling or cooling issues.
            
            **Recommended Actions:**
            - Check cooling system
            - Monitor ambient temperature
            - Consider reducing CPU load
            - Check for dust buildup
        labels:
          severity: warning
          team: infrastructure
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              expr: "capsim_events_insert_rate_per_minute > 100"
              instant: false
              range: true
              refId: A

      - uid: io-wait-high
        title: "ðŸ’¾ High IO Wait"
        condition: A
        for: 1m
        noDataState: OK
        execErrState: Error
        annotations:
          summary: "High IO wait detected ({{ $values.A.Value }}%)"
          description: |
            IO wait is {{ $values.A.Value }}%, which is above the threshold of 25%.
            
            This indicates disk I/O bottlenecks.
            
            **Recommended Actions:**
            - Check disk usage and I/O patterns
            - Consider SSD upgrade
            - Optimize database queries
            - Monitor WAL activity
        labels:
          severity: warning
          team: infrastructure
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: "rate(capsim_http_requests_total[5m]) > 50"
              instant: false
              range: true
              refId: A

      - uid: wal-size-growth
        title: "ðŸ“ WAL Size Growth"
        condition: A
        for: 1m
        noDataState: OK
        execErrState: Error
        annotations:
          summary: "High WAL growth detected ({{ $values.A.Value | humanize1024 }}B in 5m)"
          description: |
            WAL size has grown by {{ $values.A.Value | humanize1024 }}B in the last 5 minutes.
            
            This indicates high database write activity.
            
            **Recommended Actions:**
            - Monitor checkpoint frequency
            - Consider increasing checkpoint_timeout
            - Adjust wal_buffers if needed
            - Check for bulk operations
        labels:
          severity: warning
          team: database
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: "increase(capsim_events_table_inserts_total[5m]) > 1000"
              instant: false
              range: true
              refId: A

      - uid: simulation-overload
        title: "ðŸš€ Simulation Overload"
        condition: A
        for: 30s
        noDataState: OK
        execErrState: Alerting
        annotations:
          summary: "Simulation overload detected ({{ $values.A.Value }} active simulations)"
          description: |
            {{ $values.A.Value }} simulations are currently active, which may cause performance issues.
            
            **Recommended Actions:**
            - Monitor system resources
            - Consider scaling up
            - Check for stuck simulations
            - Review simulation parameters
        labels:
          severity: critical
          team: development
        data:
          - refId: A
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: prometheus
            model:
              expr: "capsim_simulations_active > 5"
              instant: false
              range: true
              refId: A

      - uid: http-request-rate-low
        title: "ðŸ“¡ Low HTTP Request Rate"
        condition: A
        for: 2m
        noDataState: OK
        execErrState: Error
        annotations:
          summary: "Low HTTP request rate detected ({{ $values.A.Value | humanizePercentage }} req/sec)"
          description: |
            HTTP request rate is {{ $values.A.Value | humanizePercentage }} req/sec, which is unusually low.
            
            This might indicate:
            - Application performance issues
            - Reduced user activity (normal)
            - Potential service degradation
            
            **Check:**
            - Application logs for errors
            - Database connection status
            - System resource utilization
        labels:
          severity: info
          team: development
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: "rate(capsim_http_requests_total[5m]) < 1"
              instant: false
              range: true
              refId: A

      - uid: memory-pressure
        title: ðŸ§  Memory Pressure
        condition: A
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: macos_memory_free_pages
              intervalMs: 1000
              maxDataPoints: 43200
              refId: A
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          description: |
            Free memory pages have dropped to {{ $values.A.Value }}, indicating memory pressure.
            
            **Recommended Actions:**
            - Monitor for memory leaks
            - Consider adjusting PostgreSQL memory settings
            - Check for memory-intensive operations
            - Review Docker container limits
          summary: Low free memory detected ({{ $values.A.Value }} pages)
        labels:
          severity: warning
          component: system
        condition: A < 1000

      - uid: system-load-high
        title: âš¡ High System Load
        condition: A
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: macos_load_average_1m
              intervalMs: 1000
              maxDataPoints: 43200
              refId: A
        noDataState: OK
        execErrState: Alerting
        for: 3m
        annotations:
          description: |
            System load average (1m) is {{ $values.A.Value }}, which is high for this system.
            
            **Potential Causes:**
            - CPU-intensive operations
            - I/O bottlenecks  
            - Too many concurrent processes
            
            **Actions:**
            - Check running processes
            - Monitor CPU and I/O usage
            - Consider performance tuning
          summary: High system load detected ({{ $values.A.Value }})
        labels:
          severity: warning
          component: system
          tuning_trigger: "true"
        condition: A > 2.0 